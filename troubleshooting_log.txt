# Azure Functions BigQuery → CosmosDB 모듈화 트러블슈팅 로그

## Metadata
- Timestamp: 2024-12-19 15:30:00 KST
- Severity: Low
- Impacted Systems: storeToCosmos Azure Function
- Tags: 모듈화, Azure Functions, BigQuery, CosmosDB, 코드 리팩토링

## Problem Summary
기존 Azure Functions 코드가 단일 파일(247줄)에 모든 로직이 집중되어 있어 유지보수와 확장성이 어려운 상황이었습니다. 코드의 가독성과 재사용성을 개선하기 위해 모듈화가 필요했습니다.

## Root Cause
- 단일 파일에 모든 기능이 집중되어 코드 복잡도가 높음
- 설정, 클라이언트 초기화, 쿼리, 데이터 처리 로직이 분리되지 않음
- 테스트와 디버깅이 어려운 구조
- 새로운 기능 추가 시 기존 코드 수정이 필요

## Resolution Steps
1. **설정 분리**: `config.py` 모듈 생성으로 환경 변수와 설정 관리
2. **클라이언트 관리**: `clients.py`에 `ClientManager` 클래스로 BigQuery/CosmosDB 클라이언트 초기화
3. **쿼리 관리**: `queries.py`에 `BigQueryQueries` 클래스로 쿼리 정의
4. **데이터 처리**: `data_processors.py`에 `DataProcessor` 클래스로 데이터 변환 로직
5. **모델 정의**: `models.py`에 dataclass를 사용한 데이터 모델 정의
6. **유틸리티**: `utils.py`에 성능 모니터링, 에러 처리, 로깅 유틸리티
7. **메인 함수 간소화**: `__init__.py`를 35줄로 간소화
8. **시간대 오류 수정**: `time_utils.py`에서 pytz 사용으로 UTC 시간대 오류 해결
9. **안전한 속성 접근**: 모든 row 속성 접근을 getattr()로 안전하게 변경하여 AttributeError 방지
10. **환경 변수 검증**: config.py에 환경 변수 검증 로직 추가
11. **에러 처리 강화**: 개별 행 처리 오류 시에도 전체 프로세스 계속 진행
12. **BigQuery Iterator 오류 수정**: RowIterator 중복 순회 문제 해결

## Error Message / Logs
**발견된 오류 1:**
```
'No time zone found with key UTC'
행 처리 중 오류 발생 (visitorId: 64ff6c97-a13a-4442-816d-0e29a23095b1): 'No time zone found with key UTC'
```

**발견된 오류 2:**
```
('Iterator has already started', <google.cloud.bigquery.table.RowIterator object at 0x00000161A40FAC10>)
```

**해결 방법:**
1. time_utils.py에서 zoneinfo 대신 pytz 사용
2. requirements.txt에 pytz 패키지 추가
3. 모든 row 속성 접근을 getattr()로 안전하게 변경
4. BigQuery RowIterator 중복 순회 문제 해결 (len(list(rows)) 제거)

**기존 코드 구조:**
```python
# 247줄의 단일 파일
import os, logging, json, uuid, azure.functions as func
from google.cloud import bigquery
from google.oauth2 import service_account
from azure.cosmos import CosmosClient, PartitionKey
# ... 모든 로직이 하나의 파일에 집중
```

**모듈화 후 구조:**
```python
# __init__.py (35줄)
from .clients import client_manager
from .queries import BigQueryQueries
from .data_processors import DataProcessor
# 깔끔한 메인 함수
```

## Related Commits or Pull Requests
- 모듈화 작업: 8개 파일 생성 (config.py, clients.py, queries.py, data_processors.py, models.py, utils.py, README.md)
- 기존 __init__.py 파일 리팩토링

## Reproduction Steps
1. 기존 단일 파일 구조 확인
2. 각 기능별로 모듈 분리 계획 수립
3. 의존성 관계 분석 후 모듈 생성 순서 결정
4. 각 모듈별 클래스 및 함수 정의
5. 메인 함수에서 모듈 import하여 사용

## Prevention / Lessons Learned
- Azure Functions에서도 모듈화가 가능하고 권장됨
- 설정과 로직을 분리하면 환경별 배포가 용이
- 클래스 기반 구조로 테스트 작성이 쉬워짐
- 유틸리티 함수 분리로 코드 재사용성 향상
- README 작성으로 프로젝트 이해도 증대

## Related Links
- Azure Functions Python 개발 가이드
- Google Cloud BigQuery Python 클라이언트 문서
- Azure CosmosDB Python SDK 문서

---

## Metadata
- Timestamp: 2024-12-19 16:45:00 KST
- Severity: Critical
- Impacted Systems: GitHub Repository, Git Push Process
- Tags: 보안, GitHub, 서비스 계정 키, 푸시 보호

## Problem Summary
GitHub에 코드를 푸시할 때 서비스 계정 키 파일(service_account_key.json)이 보안 위반으로 감지되어 푸시가 차단되었습니다. GitHub의 Push Protection 기능이 Google Cloud 서비스 계정 자격 증명을 감지하여 푸시를 거부했습니다.

## Root Cause
- 서비스 계정 키 파일이 Git 저장소에 포함되어 커밋됨
- GitHub의 자동 보안 스캔이 민감한 자격 증명을 감지
- .gitignore에 파일이 추가되었지만 이미 커밋 히스토리에 포함된 상태
- Push Protection이 활성화되어 있어 보안 위반 시 푸시 차단

## Resolution Steps
1. **파일 추적 제거**: `git rm --cached service_account_key.json`로 Git 추적에서 제거
2. **커밋 히스토리 정리**: `git filter-branch`를 사용하여 커밋 히스토리에서 민감한 파일 완전 제거
3. **강제 푸시**: `git push --force`로 정리된 히스토리로 원격 저장소 업데이트
4. **보안 확인**: GitHub에서 보안 위반 해제 확인

## Error Message / Logs
```
remote: error: GH013: Repository rule violations found for refs/heads/main.
remote: 
remote: - GITHUB PUSH PROTECTION
remote:   —————————————————————————————————————————
remote:     Resolve the following violations before pushing again
remote:
remote:     - Push cannot contain secrets
remote:
remote:       —— Google Cloud Service Account Credentials ——————————
remote:        locations:
remote:          - commit: 5103ff8d86d1f0f829aa40ceec98cc0252333155
remote:            path: service_account_key.json:1
```

## Related Commits or Pull Requests
- 커밋 5103ff8d86d1f0f829aa40ceec98cc0252333155에서 서비스 계정 키 포함
- .gitignore 업데이트로 향후 민감한 파일 추적 방지

## Reproduction Steps
1. 민감한 파일(API 키, 서비스 계정 키 등)을 Git 저장소에 추가
2. GitHub에 푸시 시도
3. Push Protection이 자동으로 보안 위반 감지
4. 푸시 거부 및 보안 경고 표시

## Prevention / Lessons Learned
- 민감한 파일은 항상 .gitignore에 추가 후 커밋
- 서비스 계정 키는 환경 변수나 Azure Key Vault 사용 권장
- GitHub의 Push Protection 기능 활용으로 보안 강화
- 커밋 전에 민감한 정보 포함 여부 확인 필요
- 정기적인 보안 스캔으로 저장소 보안 상태 점검

## Related Links
- GitHub Push Protection 문서
- Google Cloud 서비스 계정 보안 가이드
- Azure Key Vault 서비스 계정 키 관리 

---

## Metadata
- Timestamp: 2025-07-12 16:42:00 KST
- Severity: Medium
- Impacted Systems: storeToCosmos Azure Function, Azure SQL Database
- Tags: ODBC, SQL Server, 로컬 개발, 드라이버, 테이블 스키마

## Problem Summary
Azure Functions 로컬 개발 환경에서 CosmosDB 대신 Azure SQL Database로 변경한 후, 로컬에서 `func start` 실행 시 ODBC 드라이버 관련 오류와 테이블 없음 오류가 발생했습니다.

## Root Cause
- 로컬 컴퓨터에 ODBC Driver가 설치되어 있지 않음
- 코드에서 지정한 ODBC Driver 버전(17)과 설치된 버전(18) 불일치
- Azure SQL Database에 필요한 테이블과 스키마가 생성되어 있지 않음

## Resolution Steps
1. **ODBC Driver 설치**: Microsoft에서 제공하는 ODBC Driver 18 for SQL Server 설치
2. **드라이버 버전 수정**: clients.py 파일에서 연결 문자열의 드라이버 버전을 17에서 18로 변경
3. **SQL 스키마 생성**: Azure Portal의 쿼리 편집기에서 ga_data 스키마 생성
4. **SQL 테이블 생성**: 필요한 7개 테이블(Sessions, Totals, Traffic, DeviceGeo, CustomDimensions, Hits, HitsProduct) 생성

## Error Message / Logs
**ODBC 드라이버 오류:**
```
InterfaceError: ('IM002', '[IM002] [Microsoft][ODBC 드라이버 관리자] 데이터 원본 이름이 없고 기본 드라이버를 지정하지 않았습니다. (0) (SQLDriverConnect)')
```

**테이블 없음 오류:**
```
('42S02', "[42S02] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Invalid object name 'ga_data.Sessions'. (208) (SQLExecDirectW); [42S02] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Statement(s) could not be prepared. (8180)")
```

## Related Commits or Pull Requests
- clients.py 파일 수정: ODBC Driver 버전 17에서 18로 변경
- create_tables.sql 스크립트 생성: 필요한 테이블과 스키마 생성

## Reproduction Steps
1. Azure Functions 로컬 개발 환경 설정
2. Azure SQL Database 연결 정보 구성
3. 로컬에서 `func start` 실행
4. ODBC 드라이버 오류 발생 확인
5. 드라이버 설치 후 다시 실행하여 테이블 없음 오류 확인

## Prevention / Lessons Learned
- 클라우드 서비스(Azure SQL Database)를 사용하더라도 로컬 개발 환경에는 필요한 드라이버 설치 필요
- 연결 문자열에 지정된 드라이버 버전과 실제 설치된 버전 일치 필요
- 데이터베이스 스키마와 테이블은 코드 배포 전에 미리 생성 필요
- 로컬 개발과 클라우드 환경의 차이점 이해 필요

## Related Links
- Microsoft ODBC Driver for SQL Server 다운로드: https://learn.microsoft.com/ko-kr/sql/connect/odbc/download-odbc-driver-for-sql-server
- Azure SQL Database 연결 문자열 형식: https://learn.microsoft.com/ko-kr/azure/azure-sql/database/connect-query-content-reference-guide 

---

## Metadata
- Timestamp: 2023-07-13 14:25:00 KST
- Severity: Low
- Impacted Systems: storeToSQL Azure Function
- Tags: 데이터 처리, 조건부 필터링, SQL

## Problem Summary
데이터 처리 결과에서 일부 테이블(custom, products)의 데이터 개수가 다른 테이블(sessions, totals, traffic, devicegeo, hits)과 일치하지 않는 문제가 발생했습니다. 세션 데이터는 모두 1000개씩 처리되었으나, custom은 838개, products는 911개만 처리되었습니다.

## Root Cause
- `data_processors.py` 파일의 `_process_custom_data` 함수에서 customDimensions_index가 None인 경우 데이터 처리를 건너뛰도록 설정됨
- `_process_products_data` 함수에서 hits_product_v2ProductName이 None인 경우 데이터 처리를 건너뛰도록 설정됨
- 이로 인해 해당 필드가 없는 데이터는 처리되지 않아 테이블별 데이터 수의 불일치 발생

## Resolution Steps
1. `data_processors.py` 파일의 `_process_custom_data` 함수에서 조건부 필터링 제거
   ```python
   # 수정 전
   if getattr(row, 'customDimensions_index', None) is None:
       return
   ```
   
2. `_process_products_data` 함수에서 조건부 필터링 제거
   ```python
   # 수정 전
   if getattr(row, 'hits_product_v2ProductName', None) is None:
       return
   ```

3. 두 함수 모두에서 조건 검사를 제거하여 모든 데이터를 처리하도록 수정

## Error Message / Logs
```
✅ 저장 완료:
📊 총 처리된 데이터: 6749개
  • sessions: 1000개
  • totals: 1000개
  • traffic: 1000개
  • devicegeo: 1000개
  • custom: 838개
  • hits: 1000개
  • products: 911개
```

## Related Commits or Pull Requests
- data_processors.py 파일 수정: customDimensions_index 및 hits_product_v2ProductName 관련 조건부 필터링 제거

## Reproduction Steps
1. 기존 코드로 데이터 처리 실행
2. 처리 결과 확인하여 테이블별 데이터 수 불일치 확인
3. data_processors.py 파일의 조건부 필터링 코드 확인
4. 조건부 필터링 제거 후 데이터 처리 재실행

## Prevention / Lessons Learned
- 데이터 처리 시 필드 존재 여부에 따른 필터링이 필요한지 명확히 판단 필요
- 데이터 일관성이 중요한 경우, 조건부 필터링보다 모든 데이터를 처리하고 NULL 값 허용하는 방식 고려
- 데이터 처리 결과를 정기적으로 모니터링하여 예상과 다른 결과 발생 시 원인 파악 필요
- 데이터 처리 로직 변경 시 결과에 미치는 영향 사전 검토 필요

## Related Links
- Azure SQL Database NULL 값 처리 가이드
- Google Analytics 데이터 구조 문서 

---

## Metadata
- Timestamp: 2023-07-14 10:35:00 KST
- Severity: Medium
- Impacted Systems: storeToSQL Azure Function
- Tags: 데이터베이스 스키마, NULL 값 제약조건, 테이블 삭제

## Problem Summary
CustomDimensions 테이블에 NULL 값을 삽입하려고 할 때 SQL 오류가 발생했습니다. dimensionIndex 열이 NOT NULL 제약조건을 가지고 있어 NULL 값을 허용하지 않는 문제가 있었습니다.

## Root Cause
- CustomDimensions 테이블의 dimensionIndex 열이 NOT NULL 제약조건으로 설정됨
- 최근 코드 수정으로 customDimensions_index가 NULL인 데이터도 처리하도록 변경했으나 데이터베이스 스키마와 충돌
- 데이터베이스 스키마와 코드 로직 간의 불일치로 인한 오류 발생

## Resolution Steps
1. CustomDimensions 테이블을 완전히 제거하기로 결정
2. schema.sql 파일에서 CustomDimensions 테이블 생성 코드 삭제
   ```sql
   -- 삭제된 코드
   CREATE TABLE ga_data.CustomDimensions (
       cdId VARCHAR(255) PRIMARY KEY,
       hitId VARCHAR(255),
       visitorId VARCHAR(255),
       dimensionIndex INT NOT NULL,
       dimensionValue NVARCHAR(MAX)
   );
   ```

3. data_processors.py 파일에서 CustomDimensions 관련 처리 로직 제거
   - tables 딕셔너리에서 "custom" 항목 제거
   - _process_custom_data 함수를 빈 함수로 변경
   - process_row 메소드에서 _process_custom_data 함수 호출 제거

4. 데이터베이스에서 테이블 삭제 (Azure Portal 쿼리 편집기 사용)
   ```sql
   DROP TABLE IF EXISTS ga_data.CustomDimensions;
   ```

## Error Message / Logs
```
[2025-07-14T02:30:31.830Z] 배치 삽입 실패 (ga_data.CustomDimensions): ('23000', "[23000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Cannot insert the value NULL into column 'dimensionIndex', table 'dt016-test-sqldb.ga_data.CustomDimensions'; column does not allow nulls. INSERT fails. (515) (SQLExecDirectW); [23000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]The statement has been terminated. (3621)")
[2025-07-14T02:30:31.866Z] 행 처리 중 오류 발생 (fullVisitorId: 5012431536154633744, primary_key: 20170801-0000453): ('23000', "[23000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Cannot insert the value NULL into column 'dimensionIndex', table 'dt016-test-sqldb.ga_data.CustomDimensions'; column does not allow nulls. INSERT fails. (515) (SQLExecDirectW); [23000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]The statement has been terminated. (3621)")
```

## Related Commits or Pull Requests
- schema.sql 파일 수정: CustomDimensions 테이블 생성 코드 제거
- data_processors.py 파일 수정: CustomDimensions 관련 처리 로직 제거

## Reproduction Steps
1. 코드에서 NULL 값을 허용하도록 수정
2. 데이터베이스 스키마에서는 NOT NULL 제약조건이 있는 상태
3. 함수 실행 시 NULL 값을 데이터베이스에 삽입하려고 시도
4. SQL 오류 발생 확인

## Prevention / Lessons Learned
- 코드 로직과 데이터베이스 스키마 간의 일관성 유지 필요
- 데이터베이스 스키마 변경 시 관련 코드도 함께 검토 필요
- NULL 값 처리 정책을 명확히 수립하고 코드와 스키마에 일관되게 적용
- 테이블이 필요하지 않은 경우 완전히 제거하여 불필요한 오류 방지
- 데이터베이스 제약조건과 코드 로직이 충돌할 때는 둘 중 하나를 조정하거나 불필요한 기능 제거 고려

## Related Links
- SQL Server NOT NULL 제약조건 문서
- Azure SQL Database 스키마 관리 가이드 

---

## Metadata
- Timestamp: 2023-07-14 14:20:00 KST
- Severity: Low
- Impacted Systems: storeToSQL Azure Function
- Tags: 데이터베이스 스키마, 필드 추가, 데이터 확장

## Problem Summary
팀장의 요청으로 Hits 테이블에 product_productQuantity 필드를 추가해야 했습니다. 이 필드는 Google Analytics 데이터에서 제품 수량 정보를 저장하는 용도로 사용됩니다.

## Root Cause
- 기존 스키마에는 product_productQuantity 필드가 정의되어 있지 않았음
- BigQuery에서는 p.productQuantity로 데이터가 제공되지만 이를 가져오는 쿼리에 포함되어 있지 않았음
- 데이터 처리 로직에서도 해당 필드를 처리하는 코드가 없었음

## Resolution Steps
1. schema.sql 파일의 Hits 테이블 정의에 product_productQuantity 필드 추가
   ```sql
   CREATE TABLE ga_data.Hits (
       -- 기존 필드들...
       contentGroupUniqueViews2 INT,
       contentGroupUniqueViews3 INT,
       product_productQuantity INT,  -- 새로 추가된 필드
       dataSource VARCHAR(255)
   );
   ```

2. data_processors.py 파일의 _process_hits_data 함수 수정
   - hits_data 배열에 getattr(row, 'hits_product_productQuantity', None) 추가
   - columns 배열에 "product_productQuantity" 추가

3. queries.py 파일의 BigQuery 쿼리 수정
   - SELECT 절에 p.productQuantity AS hits_product_productQuantity 추가

4. 데이터베이스에 변경사항 적용 (Azure Portal 쿼리 편집기 사용)
   ```sql
   ALTER TABLE ga_data.Hits
   ADD product_productQuantity INT;
   ```

## Error Message / Logs
변경 전에는 해당 필드가 없어 데이터가 누락되었습니다.

## Related Commits or Pull Requests
- schema.sql 파일 수정: Hits 테이블에 product_productQuantity INT 필드 추가
- data_processors.py 파일 수정: _process_hits_data 함수에 product_productQuantity 처리 추가
- queries.py 파일 수정: BigQuery 쿼리에 p.productQuantity AS hits_product_productQuantity 추가

## Reproduction Steps
1. 팀장의 요청에 따라 제품 수량 정보를 저장할 필드 추가 필요 확인
2. schema.sql, data_processors.py, queries.py 파일에서 관련 코드 수정
3. 데이터베이스에 변경사항 적용
4. 변경된 코드로 함수 실행하여 제품 수량 정보가 정상적으로 저장되는지 확인

## Prevention / Lessons Learned
- 데이터베이스 스키마 변경 시 관련된 모든 코드 파일을 함께 수정해야 함
- 데이터 처리 파이프라인에서 필드 추가 시 소스(BigQuery), 처리 로직, 저장소(SQL DB) 모두 일관되게 변경 필요
- 필드 추가 시 NULL 허용 여부를 명확히 결정하고 스키마에 반영
- 스키마 변경 후 기존 데이터베이스에 변경사항 적용 필요
- 데이터 파이프라인 수정 시 전체 흐름을 고려하여 누락된 부분이 없는지 확인 필요

## Related Links
- Google Analytics 데이터 스키마 문서
- BigQuery 쿼리 최적화 가이드
- Azure SQL Database 스키마 변경 가이드 

---

## Metadata
- Timestamp: 2024-11-06 14:30:00 KST
- Severity: Medium
- Impacted Systems: Azure SQL Database, BigQuery Export
- Tags: Data Type, Schema, BigQuery

## Problem Summary
Google Analytics 데이터 스키마와 Azure SQL 데이터베이스 스키마 간의 데이터 타입 불일치 문제가 발생했습니다.

## Root Cause
Google Analytics 공식 문서에 따르면 BigQuery Export 스키마에서는 BIGINT 타입을 사용하지 않고 INTEGER 타입을 사용합니다. 그러나 우리 프로젝트의 schema.sql 파일에서는 transactionRevenue, totalTransactionRevenue, productPrice, localProductPrice 필드에 BIGINT 타입을 사용하고 있었습니다.

## Resolution Steps
1. Google Analytics 공식 문서를 확인하여 올바른 데이터 타입을 파악했습니다.
2. schema.sql 파일에서 BIGINT 타입을 사용하는 모든 필드를 INTEGER 타입으로 변경했습니다.
3. 변경된 필드:
   - ga_data.Totals 테이블의 transactionRevenue
   - ga_data.Totals 테이블의 totalTransactionRevenue
   - ga_data.HitsProduct 테이블의 productPrice
   - ga_data.HitsProduct 테이블의 localProductPrice

## Error Message / Logs
데이터 타입 불일치로 인한 명시적인 오류는 발생하지 않았으나, Google Analytics 공식 문서와의 일관성을 위해 수정이 필요했습니다.

## Related Commits or Pull Requests
N/A

## Reproduction Steps
1. 데이터베이스 스키마를 설계할 때는 항상 데이터 소스의 공식 문서를 참조하여 데이터 타입을 정확하게 맞추는 것이 중요합니다. 특히 외부 시스템과 연동할 때는 데이터 타입 불일치로 인한 문제가 발생할 수 있으므로 주의해야 합니다.

## Prevention / Lessons Learned
데이터베이스 스키마를 설계할 때는 항상 데이터 소스의 공식 문서를 참조하여 데이터 타입을 정확하게 맞추는 것이 중요합니다. 특히 외부 시스템과 연동할 때는 데이터 타입 불일치로 인한 문제가 발생할 수 있으므로 주의해야 합니다.

## Related Links
- https://support.google.com/analytics/answer/3437719?hl=en
- https://support.google.com/analytics/answer/7029846?hl=en 

---

## Metadata
- Timestamp: 2024-11-06 17:45:00 KST
- Severity: Medium
- Impacted Systems: storeToSQL Azure Function, BigQuery 쿼리
- Tags: 쿼리 업데이트, BigQuery, 데이터 처리

## Problem Summary
팀장의 요청에 따라 BigQuery 쿼리를 업데이트해야 했습니다. 기존 쿼리에서 일부 필드의 처리 방식과 데이터 형식이 변경되어야 했습니다.

## Root Cause
기존 쿼리는 일부 필드에 대한 처리가 최신 요구사항과 일치하지 않았습니다. 특히 다음과 같은 문제가 있었습니다:
- Boolean 필드(isInteraction, isEntrance, isExit, isImpression, isClick)에 IFNULL 처리가 누락됨
- 숫자 필드(totalTransactionRevenue, productPrice)에 ROUND 함수가 적용되지 않음
- eCommerceAction.action_type의 CASE 문에 'Checkout options' 옵션이 누락됨
- productRevenue 필드가 쿼리에 포함되지 않음

## Resolution Steps
1. queries.py 파일의 get_analytics_data_query 함수를 수정하여 팀장이 제공한 쿼리로 업데이트했습니다.
2. 주요 변경 사항:
   - Boolean 필드에 IFNULL 함수 적용: `IFNULL(h.isInteraction, FALSE)`, `IFNULL(h.isEntrance, FALSE)`, `IFNULL(h.isExit, FALSE)`, `IFNULL(p.isImpression, FALSE)`, `IFNULL(p.isClick, FALSE)`
   - 숫자 필드에 ROUND 함수 적용: `ROUND(t.totals.totalTransactionRevenue / 1000000, 2)`, `ROUND(p.productPrice / 1000000, 2)`
   - eCommerceAction.action_type CASE 문에 'Checkout options' 옵션 추가
   - productRevenue 필드 추가: `ROUND(p.productRevenue / 1000000, 2) AS hits_product_productRevenue`
3. data_processors.py 파일도 수정하여 새로운 필드(productRevenue)를 처리하도록 했습니다.
4. schema.sql 파일에 HitsProduct 테이블에 productRevenue 필드를 추가했습니다.

## Error Message / Logs
해당 없음

## Related Commits or Pull Requests
- queries.py 파일 수정: BigQuery 쿼리 업데이트
- data_processors.py 파일 수정: productRevenue 필드 처리 추가 및 action_type_map에 'Checkout options' 추가
- schema.sql 파일 수정: HitsProduct 테이블에 productRevenue INTEGER 필드 추가

## Reproduction Steps
1. 팀장이 제공한 쿼리를 검토
2. 기존 쿼리와 비교하여 변경 사항 파악
3. queries.py, data_processors.py, schema.sql 파일 수정
4. 변경된 코드로 함수 실행하여 정상 작동 확인

## Prevention / Lessons Learned
- 쿼리 변경 시 관련된 모든 코드 파일(데이터 처리 로직, 스키마 정의)을 함께 검토하고 수정해야 함
- 새로운 필드 추가 시 데이터 파이프라인의 모든 단계(쿼리, 처리, 저장)에서 일관되게 처리되어야 함
- NULL 값 처리와 데이터 형식 변환(ROUND 등)은 데이터 품질에 중요한 영향을 미치므로 신중하게 처리해야 함
- 쿼리 변경 후에는 실제 데이터로 테스트하여 예상대로 작동하는지 확인 필요

## Related Links
- Google Analytics BigQuery Export 스키마 문서
- Azure SQL Database 데이터 타입 가이드 

---

## Metadata
- Timestamp: 2024-11-06 18:30:00 KST
- Severity: High
- Impacted Systems: storeToSQL Azure Function, Azure SQL Database, Power BI 연동
- Tags: 데이터 모델링, 키 관계, 스키마 변경, Power BI

## Problem Summary
팀장의 요청에 따라 Power BI에서 테이블 간 관계를 명확하게 설정할 수 있도록 데이터베이스 스키마와 코드를 수정해야 했습니다. 기존 시스템에서는 테이블 간 관계가 명확하게 정의되어 있지 않아 Power BI에서 데이터 모델링이 어려웠습니다.

## Root Cause
기존 데이터베이스 스키마에서는 각 테이블이 서로 다른 형식의 키를 사용하고 있었습니다:
- Sessions, Totals, Traffic, DeviceGeo 테이블은 날짜 기반의 primary_key 사용
- Hits 테이블은 UUID 형식의 hitId 사용
- HitsProduct 테이블은 UUID 형식의 productId 사용

이로 인해 Power BI에서 테이블 간 관계를 설정하기 어려웠고, 데이터 분석이 제한되었습니다.

## Resolution Steps
1. **스키마 재설계**: 팀장의 지침에 따라 세 가지 레벨의 키 체계를 설계했습니다.
   - 세션 레벨: `session_key = CONCAT(fullVisitorId, '-', visitId)`
   - 히트 레벨: `hit_key = CONCAT(fullVisitorId, '-', visitId, '-', hitNumber)`
   - 상품 레벨: `product_hit_key = CONCAT(fullVisitorId, '-', visitId, '-', hitNumber, '-', productSKU)`

2. **schema.sql 파일 수정**:
   - Sessions, Totals, Traffic, DeviceGeo 테이블에 session_key 필드 추가 (PRIMARY KEY로 설정)
   - Hits 테이블에 hit_key(PRIMARY KEY)와 session_key 필드 추가
   - HitsProduct 테이블에 product_hit_key(PRIMARY KEY)와 hit_key 필드 추가
   - 기존 키 필드는 이전 버전 호환성을 위해 유지
   - 새로운 인덱스 추가: IX_Sessions_SessionKey, IX_Hits_SessionKey, IX_Hits_HitKey, IX_HitsProduct_HitKey

3. **queries.py 파일 수정**:
   - BigQuery 쿼리에 세 가지 새로운 키 필드 추가:
     - `CONCAT(fullVisitorId, '-', CAST(visitId AS STRING)) AS session_key`
     - `CONCAT(fullVisitorId, '-', CAST(visitId AS STRING), '-', CAST(h.hitNumber AS STRING)) AS hit_key`
     - `CONCAT(fullVisitorId, '-', CAST(visitId AS STRING), '-', CAST(h.hitNumber AS STRING), '-', IFNULL(p.productSKU, 'null')) AS product_hit_key`
   - productSKU 필드도 추가: `p.productSKU AS hits_product_productSKU`

4. **data_processors.py 파일 수정**:
   - process_row 메서드에서 새로운 키 필드를 가져오도록 수정
   - 세션 처리 로직을 primary_key 대신 session_key 기반으로 변경
   - 모든 처리 메서드의 매개변수와 데이터 배열, 컬럼 리스트 수정
   - 이전 버전 호환성을 위해 UUID 생성 로직 유지

## Error Message / Logs
해당 없음

## Related Commits or Pull Requests
- schema.sql 파일 수정: 새로운 키 필드 추가 및 PRIMARY KEY 변경
- queries.py 파일 수정: 새로운 키 필드 생성 쿼리 추가
- data_processors.py 파일 수정: 새로운 키 필드 처리 로직 추가

## Reproduction Steps
1. 팀장의 지침 검토: 세션, 히트, 상품 레벨별 키 체계 설계
2. schema.sql 파일에서 테이블 구조 수정
3. queries.py 파일에서 BigQuery 쿼리 수정
4. data_processors.py 파일에서 데이터 처리 로직 수정
5. 변경된 코드로 데이터 처리 테스트

## Prevention / Lessons Learned
- 데이터베이스 설계 시 데이터 모델링과 분석 도구(Power BI 등)의 요구사항을 초기부터 고려해야 함
- 테이블 간 관계를 명확하게 정의하는 것이 데이터 분석과 시각화에 중요함
- 키 체계를 설계할 때 데이터의 계층 구조(세션 > 히트 > 상품)를 반영해야 함
- 기존 시스템과의 호환성을 위해 이전 키 필드를 유지하면서 새로운 키 체계를 도입하는 방식이 효과적임
- 인덱스를 적절히 설정하여 관계 기반 쿼리의 성능을 최적화해야 함

## Related Links
- Power BI 데이터 모델링 가이드
- Azure SQL Database 관계형 데이터 모델 설계 가이드 