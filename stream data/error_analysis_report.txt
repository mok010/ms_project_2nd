# Google Analytics 데이터 파이프라인 오류 분석 보고서

팀장님께,

현재 발생 중인 Azure SQL Database 삽입 오류에 대한 분석 결과를 보고드립니다. 로그를 분석한 결과, 명확한 원인과 해결 방안을 파악했습니다.

## 오류 로그 해석

제공된 로그는 `ga_data.Hits` 테이블에 데이터 삽입 시 발생하는 PRIMARY KEY 제약 조건 위반 오류를 보여줍니다:

```
[2025-07-14T09:29:20.305Z] 배치 삽입 실패 (ga_data.Hits): ('23000', "Violation of PRIMARY KEY constraint 'PK__Hits__EB1110941D95E9DE'. Cannot insert duplicate key in object 'ga_data.Hits'. The duplicate key value is (3117083853988400603-1501574189-6)...")
```

주요 특징:
1. 동일한 `hit_key` 값이 반복적으로 삽입 시도됨 (예: `3117083853988400603-1501574189-6`)
2. 동일한 세션(`3117083853988400603-1501574189`)에서 여러 히트 데이터가 중복 처리됨
3. 로그에서 확인된 중복 hit_key 값: 
   - `3117083853988400603-1501574189-6` (114번 행)
   - `3117083853988400603-1501574189-11` (115번 행)

## 원인 분석

이 오류는 다음과 같은 원인으로 발생하고 있습니다:

1. **데이터 구조 관련 이슈**:
   - 팀장님께서 제안하신 키 구조(session_key, hit_key, product_hit_key)는 논리적으로 올바르나, 구현 과정에서 중복 처리 로직이 누락됨
   - BigQuery에서 UNNEST 연산을 사용할 때 하나의 히트가 여러 행으로 확장되는 현상 발생

2. **코드 구현 이슈**:
   - `DataProcessor` 클래스에서 세션 레벨 중복 처리는 `self.processed_session_keys` 세트로 구현되어 있으나, 히트 레벨에서는 중복 처리 로직이 없음
   - 현재 구현은 동일한 세션 내에서 동일한 히트 번호를 가진 데이터가 여러 번 처리되는 것을 방지하지 못함

## 발생 시나리오

로그를 분석한 결과, 다음과 같은 시나리오로 오류가 발생했을 것으로 추정됩니다:

1. **시나리오 1: 제품 데이터 확장으로 인한 중복**
   ```
   세션: 3117083853988400603-1501574189
     - 히트 6: 제품 A
     - 히트 6: 제품 B (동일 히트에 다른 제품)
     - 히트 11: 제품 C
     - 히트 11: 제품 D (동일 히트에 다른 제품)
   ```
   
   BigQuery에서 이 데이터를 조회할 때, 히트 6과 히트 11이 각각 두 번씩 행으로 확장되어 총 4개의 행이 생성됩니다. 현재 코드는 이러한 중복을 처리하지 못해 동일한 hit_key로 여러 번 삽입을 시도합니다.

2. **시나리오 2: 데이터 중복 조회**
   
   BigQuery 쿼리 자체에서 동일한 히트 데이터가 중복으로 조회될 가능성도 있습니다. 특히 UNNEST 연산과 JOIN 조건에 따라 데이터가 의도치 않게 복제될 수 있습니다.

## 상세 분석

로그를 더 자세히 살펴보면:

1. 행 114와 행 115에서 동일한 세션(`3117083853988400603-1501574189`)에 대한 히트 데이터가 중복 처리되고 있습니다.
2. 성공적으로 삽입된 레코드도 있습니다:
   ```
   [2025-07-14T09:29:20.423Z] 1개 레코드가 ga_data.Hits에 성공적으로 삽입됨
   [2025-07-14T09:29:20.465Z] 1개 레코드가 ga_data.HitsProduct에 성공적으로 삽입됨
   ```
   이는 첫 번째 히트 데이터는 성공적으로 삽입되었으나, 이후 동일한 hit_key를 가진 데이터 삽입 시도에서 오류가 발생했음을 의미합니다.

3. 두 가지 다른 hit_key에서 동일한 문제가 발생했습니다:
   - `3117083853988400603-1501574189-6`
   - `3117083853988400603-1501574189-11`
   
   이는 문제가 특정 데이터에 국한된 것이 아니라 데이터 처리 로직의 구조적 문제임을 시사합니다.

## 해결 방안

이 문제를 해결하기 위한 방안은 다음과 같습니다:

1. **즉시 적용 가능한 해결책**:
   - `DataProcessor` 클래스에 히트 키 중복 처리를 위한 세트 추가:
     ```python
     self.processed_hit_keys = set()
     ```
   - `_process_hits_data` 메서드에 중복 처리 방지 로직 추가:
     ```python
     if hit_key and hit_key in self.processed_hit_keys:
         logging.info(f"중복된 hit_key 건너뛰기: {hit_key}")
         return
     # 처리 후
     self.processed_hit_keys.add(hit_key)
     ```

2. **장기적 해결책**:
   - BigQuery 쿼리 수정: 동일한 hit_key를 가진 행을 하나만 선택하도록 쿼리 최적화
   - UPSERT 패턴 적용: 중복 키 충돌을 방지하는 SQL 패턴 도입
   - 데이터 모델 재검토: 히트와 제품 정보의 관계를 명확히 하여 중복 발생 가능성 최소화

## 결론

현재 발생하는 오류는 팀장님께서 제안하신 키 구조 자체의 문제가 아니라, 이를 구현하는 과정에서 히트 레벨의 중복 처리 로직이 누락된 것이 원인입니다. 위 해결 방안을 적용하면 현재의 키 구조를 유지하면서도 중복 키 오류를 해결할 수 있을 것입니다.

빠른 시일 내에 수정 작업을 진행하여 안정적인 데이터 파이프라인을 구축하겠습니다.

감사합니다. 