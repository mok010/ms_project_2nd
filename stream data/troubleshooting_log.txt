# Azure Functions BigQuery → CosmosDB 모듈화 트러블슈팅 로그

## Metadata
- Timestamp: 2024-12-19 15:30:00 KST
- Severity: Low
- Impacted Systems: storeToCosmos Azure Function
- Tags: 모듈화, Azure Functions, BigQuery, CosmosDB, 코드 리팩토링

## Problem Summary
기존 Azure Functions 코드가 단일 파일(247줄)에 모든 로직이 집중되어 있어 유지보수와 확장성이 어려운 상황이었습니다. 코드의 가독성과 재사용성을 개선하기 위해 모듈화가 필요했습니다.

## Root Cause
- 단일 파일에 모든 기능이 집중되어 코드 복잡도가 높음
- 설정, 클라이언트 초기화, 쿼리, 데이터 처리 로직이 분리되지 않음
- 테스트와 디버깅이 어려운 구조
- 새로운 기능 추가 시 기존 코드 수정이 필요

## Resolution Steps
1. **설정 분리**: `config.py` 모듈 생성으로 환경 변수와 설정 관리
2. **클라이언트 관리**: `clients.py`에 `ClientManager` 클래스로 BigQuery/CosmosDB 클라이언트 초기화
3. **쿼리 관리**: `queries.py`에 `BigQueryQueries` 클래스로 쿼리 정의
4. **데이터 처리**: `data_processors.py`에 `DataProcessor` 클래스로 데이터 변환 로직
5. **모델 정의**: `models.py`에 dataclass를 사용한 데이터 모델 정의
6. **유틸리티**: `utils.py`에 성능 모니터링, 에러 처리, 로깅 유틸리티
7. **메인 함수 간소화**: `__init__.py`를 35줄로 간소화
8. **시간대 오류 수정**: `time_utils.py`에서 pytz 사용으로 UTC 시간대 오류 해결
9. **안전한 속성 접근**: 모든 row 속성 접근을 getattr()로 안전하게 변경하여 AttributeError 방지
10. **환경 변수 검증**: config.py에 환경 변수 검증 로직 추가
11. **에러 처리 강화**: 개별 행 처리 오류 시에도 전체 프로세스 계속 진행
12. **BigQuery Iterator 오류 수정**: RowIterator 중복 순회 문제 해결

## Error Message / Logs
**발견된 오류 1:**
```
'No time zone found with key UTC'
행 처리 중 오류 발생 (visitorId: 64ff6c97-a13a-4442-816d-0e29a23095b1): 'No time zone found with key UTC'
```

**발견된 오류 2:**
```
('Iterator has already started', <google.cloud.bigquery.table.RowIterator object at 0x00000161A40FAC10>)
```

**해결 방법:**
1. time_utils.py에서 zoneinfo 대신 pytz 사용
2. requirements.txt에 pytz 패키지 추가
3. 모든 row 속성 접근을 getattr()로 안전하게 변경
4. BigQuery RowIterator 중복 순회 문제 해결 (len(list(rows)) 제거)

**기존 코드 구조:**
```python
# 247줄의 단일 파일
import os, logging, json, uuid, azure.functions as func
from google.cloud import bigquery
from google.oauth2 import service_account
from azure.cosmos import CosmosClient, PartitionKey
# ... 모든 로직이 하나의 파일에 집중
```

**모듈화 후 구조:**
```python
# __init__.py (35줄)
from .clients import client_manager
from .queries import BigQueryQueries
from .data_processors import DataProcessor
# 깔끔한 메인 함수
```

## Related Commits or Pull Requests
- 모듈화 작업: 8개 파일 생성 (config.py, clients.py, queries.py, data_processors.py, models.py, utils.py, README.md)
- 기존 __init__.py 파일 리팩토링

## Reproduction Steps
1. 기존 단일 파일 구조 확인
2. 각 기능별로 모듈 분리 계획 수립
3. 의존성 관계 분석 후 모듈 생성 순서 결정
4. 각 모듈별 클래스 및 함수 정의
5. 메인 함수에서 모듈 import하여 사용

## Prevention / Lessons Learned
- Azure Functions에서도 모듈화가 가능하고 권장됨
- 설정과 로직을 분리하면 환경별 배포가 용이
- 클래스 기반 구조로 테스트 작성이 쉬워짐
- 유틸리티 함수 분리로 코드 재사용성 향상
- README 작성으로 프로젝트 이해도 증대

## Related Links
- Azure Functions Python 개발 가이드
- Google Cloud BigQuery Python 클라이언트 문서
- Azure CosmosDB Python SDK 문서

---

## Metadata
- Timestamp: 2024-12-19 16:45:00 KST
- Severity: Critical
- Impacted Systems: GitHub Repository, Git Push Process
- Tags: 보안, GitHub, 서비스 계정 키, 푸시 보호

## Problem Summary
GitHub에 코드를 푸시할 때 서비스 계정 키 파일(service_account_key.json)이 보안 위반으로 감지되어 푸시가 차단되었습니다. GitHub의 Push Protection 기능이 Google Cloud 서비스 계정 자격 증명을 감지하여 푸시를 거부했습니다.

## Root Cause
- 서비스 계정 키 파일이 Git 저장소에 포함되어 커밋됨
- GitHub의 자동 보안 스캔이 민감한 자격 증명을 감지
- .gitignore에 파일이 추가되었지만 이미 커밋 히스토리에 포함된 상태
- Push Protection이 활성화되어 있어 보안 위반 시 푸시 차단

## Resolution Steps
1. **파일 추적 제거**: `git rm --cached service_account_key.json`로 Git 추적에서 제거
2. **커밋 히스토리 정리**: `git filter-branch`를 사용하여 커밋 히스토리에서 민감한 파일 완전 제거
3. **강제 푸시**: `git push --force`로 정리된 히스토리로 원격 저장소 업데이트
4. **보안 확인**: GitHub에서 보안 위반 해제 확인

## Error Message / Logs
```
remote: error: GH013: Repository rule violations found for refs/heads/main.
remote: 
remote: - GITHUB PUSH PROTECTION
remote:   —————————————————————————————————————————
remote:     Resolve the following violations before pushing again
remote:
remote:     - Push cannot contain secrets
remote:
remote:       —— Google Cloud Service Account Credentials ——————————
remote:        locations:
remote:          - commit: 5103ff8d86d1f0f829aa40ceec98cc0252333155
remote:            path: service_account_key.json:1
```

## Related Commits or Pull Requests
- 커밋 5103ff8d86d1f0f829aa40ceec98cc0252333155에서 서비스 계정 키 포함
- .gitignore 업데이트로 향후 민감한 파일 추적 방지

## Reproduction Steps
1. 민감한 파일(API 키, 서비스 계정 키 등)을 Git 저장소에 추가
2. GitHub에 푸시 시도
3. Push Protection이 자동으로 보안 위반 감지
4. 푸시 거부 및 보안 경고 표시

## Prevention / Lessons Learned
- 민감한 파일은 항상 .gitignore에 추가 후 커밋
- 서비스 계정 키는 환경 변수나 Azure Key Vault 사용 권장
- GitHub의 Push Protection 기능 활용으로 보안 강화
- 커밋 전에 민감한 정보 포함 여부 확인 필요
- 정기적인 보안 스캔으로 저장소 보안 상태 점검

## Related Links
- GitHub Push Protection 문서
- Google Cloud 서비스 계정 보안 가이드
- Azure Key Vault 서비스 계정 키 관리 

---

## Metadata
- Timestamp: 2025-07-12 16:42:00 KST
- Severity: Medium
- Impacted Systems: storeToCosmos Azure Function, Azure SQL Database
- Tags: ODBC, SQL Server, 로컬 개발, 드라이버, 테이블 스키마

## Problem Summary
Azure Functions 로컬 개발 환경에서 CosmosDB 대신 Azure SQL Database로 변경한 후, 로컬에서 `func start` 실행 시 ODBC 드라이버 관련 오류와 테이블 없음 오류가 발생했습니다.

## Root Cause
- 로컬 컴퓨터에 ODBC Driver가 설치되어 있지 않음
- 코드에서 지정한 ODBC Driver 버전(17)과 설치된 버전(18) 불일치
- Azure SQL Database에 필요한 테이블과 스키마가 생성되어 있지 않음

## Resolution Steps
1. **ODBC Driver 설치**: Microsoft에서 제공하는 ODBC Driver 18 for SQL Server 설치
2. **드라이버 버전 수정**: clients.py 파일에서 연결 문자열의 드라이버 버전을 17에서 18로 변경
3. **SQL 스키마 생성**: Azure Portal의 쿼리 편집기에서 ga_data 스키마 생성
4. **SQL 테이블 생성**: 필요한 7개 테이블(Sessions, Totals, Traffic, DeviceGeo, CustomDimensions, Hits, HitsProduct) 생성

## Error Message / Logs
**ODBC 드라이버 오류:**
```
InterfaceError: ('IM002', '[IM002] [Microsoft][ODBC 드라이버 관리자] 데이터 원본 이름이 없고 기본 드라이버를 지정하지 않았습니다. (0) (SQLDriverConnect)')
```

**테이블 없음 오류:**
```
('42S02', "[42S02] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Invalid object name 'ga_data.Sessions'. (208) (SQLExecDirectW); [42S02] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Statement(s) could not be prepared. (8180)")
```

## Related Commits or Pull Requests
- clients.py 파일 수정: ODBC Driver 버전 17에서 18로 변경
- create_tables.sql 스크립트 생성: 필요한 테이블과 스키마 생성

## Reproduction Steps
1. Azure Functions 로컬 개발 환경 설정
2. Azure SQL Database 연결 정보 구성
3. 로컬에서 `func start` 실행
4. ODBC 드라이버 오류 발생 확인
5. 드라이버 설치 후 다시 실행하여 테이블 없음 오류 확인

## Prevention / Lessons Learned
- 클라우드 서비스(Azure SQL Database)를 사용하더라도 로컬 개발 환경에는 필요한 드라이버 설치 필요
- 연결 문자열에 지정된 드라이버 버전과 실제 설치된 버전 일치 필요
- 데이터베이스 스키마와 테이블은 코드 배포 전에 미리 생성 필요
- 로컬 개발과 클라우드 환경의 차이점 이해 필요

## Related Links
- Microsoft ODBC Driver for SQL Server 다운로드: https://learn.microsoft.com/ko-kr/sql/connect/odbc/download-odbc-driver-for-sql-server
- Azure SQL Database 연결 문자열 형식: https://learn.microsoft.com/ko-kr/azure/azure-sql/database/connect-query-content-reference-guide 

---

## Metadata
- Timestamp: 2023-07-13 14:25:00 KST
- Severity: Low
- Impacted Systems: storeToSQL Azure Function
- Tags: 데이터 처리, 조건부 필터링, SQL

## Problem Summary
데이터 처리 결과에서 일부 테이블(custom, products)의 데이터 개수가 다른 테이블(sessions, totals, traffic, devicegeo, hits)과 일치하지 않는 문제가 발생했습니다. 세션 데이터는 모두 1000개씩 처리되었으나, custom은 838개, products는 911개만 처리되었습니다.

## Root Cause
- `data_processors.py` 파일의 `_process_custom_data` 함수에서 customDimensions_index가 None인 경우 데이터 처리를 건너뛰도록 설정됨
- `_process_products_data` 함수에서 hits_product_v2ProductName이 None인 경우 데이터 처리를 건너뛰도록 설정됨
- 이로 인해 해당 필드가 없는 데이터는 처리되지 않아 테이블별 데이터 수의 불일치 발생

## Resolution Steps
1. `data_processors.py` 파일의 `_process_custom_data` 함수에서 조건부 필터링 제거
   ```python
   # 수정 전
   if getattr(row, 'customDimensions_index', None) is None:
       return
   ```
   
2. `_process_products_data` 함수에서 조건부 필터링 제거
   ```python
   # 수정 전
   if getattr(row, 'hits_product_v2ProductName', None) is None:
       return
   ```

3. 두 함수 모두에서 조건 검사를 제거하여 모든 데이터를 처리하도록 수정

## Error Message / Logs
```
✅ 저장 완료:
📊 총 처리된 데이터: 6749개
  • sessions: 1000개
  • totals: 1000개
  • traffic: 1000개
  • devicegeo: 1000개
  • custom: 838개
  • hits: 1000개
  • products: 911개
```

## Related Commits or Pull Requests
- data_processors.py 파일 수정: customDimensions_index 및 hits_product_v2ProductName 관련 조건부 필터링 제거

## Reproduction Steps
1. 기존 코드로 데이터 처리 실행
2. 처리 결과 확인하여 테이블별 데이터 수 불일치 확인
3. data_processors.py 파일의 조건부 필터링 코드 확인
4. 조건부 필터링 제거 후 데이터 처리 재실행

## Prevention / Lessons Learned
- 데이터 처리 시 필드 존재 여부에 따른 필터링이 필요한지 명확히 판단 필요
- 데이터 일관성이 중요한 경우, 조건부 필터링보다 모든 데이터를 처리하고 NULL 값 허용하는 방식 고려
- 데이터 처리 결과를 정기적으로 모니터링하여 예상과 다른 결과 발생 시 원인 파악 필요
- 데이터 처리 로직 변경 시 결과에 미치는 영향 사전 검토 필요

## Related Links
- Azure SQL Database NULL 값 처리 가이드
- Google Analytics 데이터 구조 문서 

---

## Metadata
- Timestamp: 2023-07-14 10:35:00 KST
- Severity: Medium
- Impacted Systems: storeToSQL Azure Function
- Tags: 데이터베이스 스키마, NULL 값 제약조건, 테이블 삭제

## Problem Summary
CustomDimensions 테이블에 NULL 값을 삽입하려고 할 때 SQL 오류가 발생했습니다. dimensionIndex 열이 NOT NULL 제약조건을 가지고 있어 NULL 값을 허용하지 않는 문제가 있었습니다.

## Root Cause
- CustomDimensions 테이블의 dimensionIndex 열이 NOT NULL 제약조건으로 설정됨
- 최근 코드 수정으로 customDimensions_index가 NULL인 데이터도 처리하도록 변경했으나 데이터베이스 스키마와 충돌
- 데이터베이스 스키마와 코드 로직 간의 불일치로 인한 오류 발생

## Resolution Steps
1. CustomDimensions 테이블을 완전히 제거하기로 결정
2. schema.sql 파일에서 CustomDimensions 테이블 생성 코드 삭제
   ```sql
   -- 삭제된 코드
   CREATE TABLE ga_data.CustomDimensions (
       cdId VARCHAR(255) PRIMARY KEY,
       hitId VARCHAR(255),
       visitorId VARCHAR(255),
       dimensionIndex INT NOT NULL,
       dimensionValue NVARCHAR(MAX)
   );
   ```

3. data_processors.py 파일에서 CustomDimensions 관련 처리 로직 제거
   - tables 딕셔너리에서 "custom" 항목 제거
   - _process_custom_data 함수를 빈 함수로 변경
   - process_row 메소드에서 _process_custom_data 함수 호출 제거

4. 데이터베이스에서 테이블 삭제 (Azure Portal 쿼리 편집기 사용)
   ```sql
   DROP TABLE IF EXISTS ga_data.CustomDimensions;
   ```

## Error Message / Logs
```
[2025-07-14T02:30:31.830Z] 배치 삽입 실패 (ga_data.CustomDimensions): ('23000', "[23000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Cannot insert the value NULL into column 'dimensionIndex', table 'dt016-test-sqldb.ga_data.CustomDimensions'; column does not allow nulls. INSERT fails. (515) (SQLExecDirectW); [23000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]The statement has been terminated. (3621)")
[2025-07-14T02:30:31.866Z] 행 처리 중 오류 발생 (fullVisitorId: 5012431536154633744, primary_key: 20170801-0000453): ('23000', "[23000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Cannot insert the value NULL into column 'dimensionIndex', table 'dt016-test-sqldb.ga_data.CustomDimensions'; column does not allow nulls. INSERT fails. (515) (SQLExecDirectW); [23000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]The statement has been terminated. (3621)")
```

## Related Commits or Pull Requests
- schema.sql 파일 수정: CustomDimensions 테이블 생성 코드 제거
- data_processors.py 파일 수정: CustomDimensions 관련 처리 로직 제거

## Reproduction Steps
1. 코드에서 NULL 값을 허용하도록 수정
2. 데이터베이스 스키마에서는 NOT NULL 제약조건이 있는 상태
3. 함수 실행 시 NULL 값을 데이터베이스에 삽입하려고 시도
4. SQL 오류 발생 확인

## Prevention / Lessons Learned
- 코드 로직과 데이터베이스 스키마 간의 일관성 유지 필요
- 데이터베이스 스키마 변경 시 관련 코드도 함께 검토 필요
- NULL 값 처리 정책을 명확히 수립하고 코드와 스키마에 일관되게 적용
- 테이블이 필요하지 않은 경우 완전히 제거하여 불필요한 오류 방지
- 데이터베이스 제약조건과 코드 로직이 충돌할 때는 둘 중 하나를 조정하거나 불필요한 기능 제거 고려

## Related Links
- SQL Server NOT NULL 제약조건 문서
- Azure SQL Database 스키마 관리 가이드 

---

## Metadata
- Timestamp: 2023-07-14 14:20:00 KST
- Severity: Low
- Impacted Systems: storeToSQL Azure Function
- Tags: 데이터베이스 스키마, 필드 추가, 데이터 확장

## Problem Summary
팀장의 요청으로 Hits 테이블에 product_productQuantity 필드를 추가해야 했습니다. 이 필드는 Google Analytics 데이터에서 제품 수량 정보를 저장하는 용도로 사용됩니다.

## Root Cause
- 기존 스키마에는 product_productQuantity 필드가 정의되어 있지 않았음
- BigQuery에서는 p.productQuantity로 데이터가 제공되지만 이를 가져오는 쿼리에 포함되어 있지 않았음
- 데이터 처리 로직에서도 해당 필드를 처리하는 코드가 없었음

## Resolution Steps
1. schema.sql 파일의 Hits 테이블 정의에 product_productQuantity 필드 추가
   ```sql
   CREATE TABLE ga_data.Hits (
       -- 기존 필드들...
       contentGroupUniqueViews2 INT,
       contentGroupUniqueViews3 INT,
       product_productQuantity INT,  -- 새로 추가된 필드
       dataSource VARCHAR(255)
   );
   ```

2. data_processors.py 파일의 _process_hits_data 함수 수정
   - hits_data 배열에 getattr(row, 'hits_product_productQuantity', None) 추가
   - columns 배열에 "product_productQuantity" 추가

3. queries.py 파일의 BigQuery 쿼리 수정
   - SELECT 절에 p.productQuantity AS hits_product_productQuantity 추가

4. 데이터베이스에 변경사항 적용 (Azure Portal 쿼리 편집기 사용)
   ```sql
   ALTER TABLE ga_data.Hits
   ADD product_productQuantity INT;
   ```

## Error Message / Logs
변경 전에는 해당 필드가 없어 데이터가 누락되었습니다.

## Related Commits or Pull Requests
- schema.sql 파일 수정: Hits 테이블에 product_productQuantity INT 필드 추가
- data_processors.py 파일 수정: _process_hits_data 함수에 product_productQuantity 처리 추가
- queries.py 파일 수정: BigQuery 쿼리에 p.productQuantity AS hits_product_productQuantity 추가

## Reproduction Steps
1. 팀장의 요청에 따라 제품 수량 정보를 저장할 필드 추가 필요 확인
2. schema.sql, data_processors.py, queries.py 파일에서 관련 코드 수정
3. 데이터베이스에 변경사항 적용
4. 변경된 코드로 함수 실행하여 제품 수량 정보가 정상적으로 저장되는지 확인

## Prevention / Lessons Learned
- 데이터베이스 스키마 변경 시 관련된 모든 코드 파일을 함께 수정해야 함
- 데이터 처리 파이프라인에서 필드 추가 시 소스(BigQuery), 처리 로직, 저장소(SQL DB) 모두 일관되게 변경 필요
- 필드 추가 시 NULL 허용 여부를 명확히 결정하고 스키마에 반영
- 스키마 변경 후 기존 데이터베이스에 변경사항 적용 필요
- 데이터 파이프라인 수정 시 전체 흐름을 고려하여 누락된 부분이 없는지 확인 필요

## Related Links
- Google Analytics 데이터 스키마 문서
- BigQuery 쿼리 최적화 가이드
- Azure SQL Database 스키마 변경 가이드 

---

## Metadata
- Timestamp: 2024-11-06 14:30:00 KST
- Severity: Medium
- Impacted Systems: Azure SQL Database, BigQuery Export
- Tags: Data Type, Schema, BigQuery

## Problem Summary
Google Analytics 데이터 스키마와 Azure SQL 데이터베이스 스키마 간의 데이터 타입 불일치 문제가 발생했습니다.

## Root Cause
Google Analytics 공식 문서에 따르면 BigQuery Export 스키마에서는 BIGINT 타입을 사용하지 않고 INTEGER 타입을 사용합니다. 그러나 우리 프로젝트의 schema.sql 파일에서는 transactionRevenue, totalTransactionRevenue, productPrice, localProductPrice 필드에 BIGINT 타입을 사용하고 있었습니다.

## Resolution Steps
1. Google Analytics 공식 문서를 확인하여 올바른 데이터 타입을 파악했습니다.
2. schema.sql 파일에서 BIGINT 타입을 사용하는 모든 필드를 INTEGER 타입으로 변경했습니다.
3. 변경된 필드:
   - ga_data.Totals 테이블의 transactionRevenue
   - ga_data.Totals 테이블의 totalTransactionRevenue
   - ga_data.HitsProduct 테이블의 productPrice
   - ga_data.HitsProduct 테이블의 localProductPrice

## Error Message / Logs
데이터 타입 불일치로 인한 명시적인 오류는 발생하지 않았으나, Google Analytics 공식 문서와의 일관성을 위해 수정이 필요했습니다.

## Related Commits or Pull Requests
N/A

## Reproduction Steps
1. 데이터베이스 스키마를 설계할 때는 항상 데이터 소스의 공식 문서를 참조하여 데이터 타입을 정확하게 맞추는 것이 중요합니다. 특히 외부 시스템과 연동할 때는 데이터 타입 불일치로 인한 문제가 발생할 수 있으므로 주의해야 합니다.

## Prevention / Lessons Learned
데이터베이스 스키마를 설계할 때는 항상 데이터 소스의 공식 문서를 참조하여 데이터 타입을 정확하게 맞추는 것이 중요합니다. 특히 외부 시스템과 연동할 때는 데이터 타입 불일치로 인한 문제가 발생할 수 있으므로 주의해야 합니다.

## Related Links
- https://support.google.com/analytics/answer/3437719?hl=en
- https://support.google.com/analytics/answer/7029846?hl=en 

---

## Metadata
- Timestamp: 2024-11-06 17:45:00 KST
- Severity: Medium
- Impacted Systems: storeToSQL Azure Function, BigQuery 쿼리
- Tags: 쿼리 업데이트, BigQuery, 데이터 처리

## Problem Summary
팀장의 요청에 따라 BigQuery 쿼리를 업데이트해야 했습니다. 기존 쿼리에서 일부 필드의 처리 방식과 데이터 형식이 변경되어야 했습니다.

## Root Cause
기존 쿼리는 일부 필드에 대한 처리가 최신 요구사항과 일치하지 않았습니다. 특히 다음과 같은 문제가 있었습니다:
- Boolean 필드(isInteraction, isEntrance, isExit, isImpression, isClick)에 IFNULL 처리가 누락됨
- 숫자 필드(totalTransactionRevenue, productPrice)에 ROUND 함수가 적용되지 않음
- eCommerceAction.action_type의 CASE 문에 'Checkout options' 옵션이 누락됨
- productRevenue 필드가 쿼리에 포함되지 않음

## Resolution Steps
1. queries.py 파일의 get_analytics_data_query 함수를 수정하여 팀장이 제공한 쿼리로 업데이트했습니다.
2. 주요 변경 사항:
   - Boolean 필드에 IFNULL 함수 적용: `IFNULL(h.isInteraction, FALSE)`, `IFNULL(h.isEntrance, FALSE)`, `IFNULL(h.isExit, FALSE)`, `IFNULL(p.isImpression, FALSE)`, `IFNULL(p.isClick, FALSE)`
   - 숫자 필드에 ROUND 함수 적용: `ROUND(t.totals.totalTransactionRevenue / 1000000, 2)`, `ROUND(p.productPrice / 1000000, 2)`
   - eCommerceAction.action_type CASE 문에 'Checkout options' 옵션 추가
   - productRevenue 필드 추가: `ROUND(p.productRevenue / 1000000, 2) AS hits_product_productRevenue`
3. data_processors.py 파일도 수정하여 새로운 필드(productRevenue)를 처리하도록 했습니다.
4. schema.sql 파일에 HitsProduct 테이블에 productRevenue 필드를 추가했습니다.

## Error Message / Logs
해당 없음

## Related Commits or Pull Requests
- queries.py 파일 수정: BigQuery 쿼리 업데이트
- data_processors.py 파일 수정: productRevenue 필드 처리 추가 및 action_type_map에 'Checkout options' 추가
- schema.sql 파일 수정: HitsProduct 테이블에 productRevenue INTEGER 필드 추가

## Reproduction Steps
1. 팀장이 제공한 쿼리를 검토
2. 기존 쿼리와 비교하여 변경 사항 파악
3. queries.py, data_processors.py, schema.sql 파일 수정
4. 변경된 코드로 함수 실행하여 정상 작동 확인

## Prevention / Lessons Learned
- 쿼리 변경 시 관련된 모든 코드 파일(데이터 처리 로직, 스키마 정의)을 함께 검토하고 수정해야 함
- 새로운 필드 추가 시 데이터 파이프라인의 모든 단계(쿼리, 처리, 저장)에서 일관되게 처리되어야 함
- NULL 값 처리와 데이터 형식 변환(ROUND 등)은 데이터 품질에 중요한 영향을 미치므로 신중하게 처리해야 함
- 쿼리 변경 후에는 실제 데이터로 테스트하여 예상대로 작동하는지 확인 필요

## Related Links
- Google Analytics BigQuery Export 스키마 문서
- Azure SQL Database 데이터 타입 가이드 

---

## Metadata
- Timestamp: 2024-11-06 18:30:00 KST
- Severity: High
- Impacted Systems: storeToSQL Azure Function, Azure SQL Database, Power BI 연동
- Tags: 데이터 모델링, 키 관계, 스키마 변경, Power BI

## Problem Summary
팀장의 요청에 따라 Power BI에서 테이블 간 관계를 명확하게 설정할 수 있도록 데이터베이스 스키마와 코드를 수정해야 했습니다. 기존 시스템에서는 테이블 간 관계가 명확하게 정의되어 있지 않아 Power BI에서 데이터 모델링이 어려웠습니다.

## Root Cause
기존 데이터베이스 스키마에서는 각 테이블이 서로 다른 형식의 키를 사용하고 있었습니다:
- Sessions, Totals, Traffic, DeviceGeo 테이블은 날짜 기반의 primary_key 사용
- Hits 테이블은 UUID 형식의 hitId 사용
- HitsProduct 테이블은 UUID 형식의 productId 사용

이로 인해 Power BI에서 테이블 간 관계를 설정하기 어려웠고, 데이터 분석이 제한되었습니다.

## Resolution Steps
1. **스키마 재설계**: 팀장의 지침에 따라 세 가지 레벨의 키 체계를 설계했습니다.
   - 세션 레벨: `session_key = CONCAT(fullVisitorId, '-', visitId)`
   - 히트 레벨: `hit_key = CONCAT(fullVisitorId, '-', visitId, '-', hitNumber)`
   - 상품 레벨: `product_hit_key = CONCAT(fullVisitorId, '-', visitId, '-', hitNumber, '-', productSKU)`

2. **schema.sql 파일 수정**:
   - Sessions, Totals, Traffic, DeviceGeo 테이블에 session_key 필드 추가 (PRIMARY KEY로 설정)
   - Hits 테이블에 hit_key(PRIMARY KEY)와 session_key 필드 추가
   - HitsProduct 테이블에 product_hit_key(PRIMARY KEY)와 hit_key 필드 추가
   - 기존 키 필드는 이전 버전 호환성을 위해 유지
   - 새로운 인덱스 추가: IX_Sessions_SessionKey, IX_Hits_SessionKey, IX_Hits_HitKey, IX_HitsProduct_HitKey

3. **queries.py 파일 수정**:
   - BigQuery 쿼리에 세 가지 새로운 키 필드 추가:
     - `CONCAT(fullVisitorId, '-', CAST(visitId AS STRING)) AS session_key`
     - `CONCAT(fullVisitorId, '-', CAST(visitId AS STRING), '-', CAST(h.hitNumber AS STRING)) AS hit_key`
     - `CONCAT(fullVisitorId, '-', CAST(visitId AS STRING), '-', CAST(h.hitNumber AS STRING), '-', IFNULL(p.productSKU, 'null')) AS product_hit_key`
   - productSKU 필드도 추가: `p.productSKU AS hits_product_productSKU`

4. **data_processors.py 파일 수정**:
   - process_row 메서드에서 새로운 키 필드를 가져오도록 수정
   - 세션 처리 로직을 primary_key 대신 session_key 기반으로 변경
   - 모든 처리 메서드의 매개변수와 데이터 배열, 컬럼 리스트 수정
   - 이전 버전 호환성을 위해 UUID 생성 로직 유지

## Error Message / Logs
해당 없음

## Related Commits or Pull Requests
- schema.sql 파일 수정: 새로운 키 필드 추가 및 PRIMARY KEY 변경
- queries.py 파일 수정: 새로운 키 필드 생성 쿼리 추가
- data_processors.py 파일 수정: 새로운 키 필드 처리 로직 추가

## Reproduction Steps
1. 팀장의 지침 검토: 세션, 히트, 상품 레벨별 키 체계 설계
2. schema.sql 파일에서 테이블 구조 수정
3. queries.py 파일에서 BigQuery 쿼리 수정
4. data_processors.py 파일에서 데이터 처리 로직 수정
5. 변경된 코드로 데이터 처리 테스트

## Prevention / Lessons Learned
- 데이터베이스 설계 시 데이터 모델링과 분석 도구(Power BI 등)의 요구사항을 초기부터 고려해야 함
- 테이블 간 관계를 명확하게 정의하는 것이 데이터 분석과 시각화에 중요함
- 키 체계를 설계할 때 데이터의 계층 구조(세션 > 히트 > 상품)를 반영해야 함
- 기존 시스템과의 호환성을 위해 이전 키 필드를 유지하면서 새로운 키 체계를 도입하는 방식이 효과적임
- 인덱스를 적절히 설정하여 관계 기반 쿼리의 성능을 최적화해야 함

## Related Links
- Power BI 데이터 모델링 가이드
- Azure SQL Database 관계형 데이터 모델 설계 가이드 

---

## Metadata
- Timestamp: 2025-07-15 10:30:00 KST
- Severity: Low
- Impacted Systems: storeToSQL Azure Function, Azure SQL Database
- Tags: 데이터 타입, SQL, 스키마, 호환성

## Problem Summary
BigQuery와 Azure SQL Database 간 데이터 타입 차이로 인해 스키마 정의 시 혼란이 발생했습니다. BigQuery에서는 문자열 데이터 타입으로 STRING을 사용하지만, Azure SQL Database에서는 VARCHAR/NVARCHAR를 사용해야 합니다.

## Root Cause
- BigQuery와 Azure SQL Database는 서로 다른 데이터베이스 시스템으로, 지원하는 데이터 타입이 다릅니다.
- BigQuery에서는 문자열 데이터를 'STRING' 타입으로 정의합니다.
- Azure SQL Database는 SQL Server 기반으로, 문자열 데이터를 'VARCHAR'(ASCII) 또는 'NVARCHAR'(유니코드) 타입으로 정의해야 합니다.
- 'STRING' 데이터 타입은 Azure SQL Database에서 지원되지 않습니다.

## Resolution Steps
1. schema.sql 파일에서 모든 문자열 데이터 타입을 Azure SQL Database에서 지원하는 형식으로 정의:
   - ASCII 문자열: VARCHAR(길이) 사용
   - 유니코드 문자열(한글 등): NVARCHAR(길이) 사용
   - 길이 제한 없는 텍스트: NVARCHAR(MAX) 사용

2. 데이터베이스 시스템 간 데이터 타입 매핑 규칙 문서화:
   - BigQuery STRING → Azure SQL VARCHAR/NVARCHAR
   - BigQuery INTEGER → Azure SQL INT
   - BigQuery FLOAT → Azure SQL FLOAT
   - BigQuery BOOLEAN → Azure SQL BIT
   - BigQuery TIMESTAMP → Azure SQL DATETIME2

## Error Message / Logs
```
Msg 2715, Level 16, State 3, Line X
Column, parameter, or variable #X: Cannot find data type STRING.
```

## Related Commits or Pull Requests
- schema.sql 파일 수정: 데이터 타입 호환성 확보

## Reproduction Steps
1. Azure SQL Database에서 'STRING' 데이터 타입을 사용하여 테이블 생성 시도
2. 데이터 타입 오류 발생 확인

## Prevention / Lessons Learned
- 데이터베이스 시스템 간 데이터 타입 차이를 이해하고 적절한 매핑 필요
- 데이터 이관 프로젝트에서는 소스와 대상 시스템의 데이터 타입 호환성 확인 필수
- Azure SQL Database(SQL Server)는 다음 공식 문서에 명시된 데이터 타입만 지원함:
  https://learn.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql?view=azuresqldb-current

## Related Links
- Microsoft 공식 문서 - SQL Server 데이터 타입: https://learn.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql?view=azuresqldb-current#character-strings
  (해당 문서의 "Character strings" 섹션에서 VARCHAR, NVARCHAR 등 문자열 데이터 타입 확인 가능)
- Microsoft 공식 문서 - SQL Server 문자열 데이터 타입: https://learn.microsoft.com/en-us/sql/t-sql/data-types/char-and-varchar-transact-sql?view=azuresqldb-current
  (해당 문서에서 VARCHAR 데이터 타입 상세 설명 확인 가능)
- Microsoft 공식 문서 - SQL Server 유니코드 문자열 데이터 타입: https://learn.microsoft.com/en-us/sql/t-sql/data-types/nchar-and-nvarchar-transact-sql?view=azuresqldb-current
  (해당 문서에서 NVARCHAR 데이터 타입 상세 설명 확인 가능)
- BigQuery 데이터 타입: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types
- Azure SQL Database 문서: https://learn.microsoft.com/en-us/azure/azure-sql/database/ 

---

## Metadata
- Timestamp: 2025-07-16 09:15:00 KST
- Severity: Medium
- Impacted Systems: storeToSQL Azure Function, Azure SQL Database
- Tags: 데이터 모델링, 스키마 최적화, 불필요 칼럼 제거

## Problem Summary
팀장의 요청에 따라 데이터베이스 테이블에서 불필요한 칼럼들을 제거해야 했습니다. 이 칼럼들은 주로 NULL 값이 많거나, 다른 칼럼으로 대체 가능하거나, 분석에 유의미한 가치가 없는 것으로 판단되었습니다.

## Root Cause
- 초기 데이터 모델링 시 Google Analytics의 모든 필드를 포함시켰으나, 일부 필드는 실제 사용되지 않음
- 일부 칼럼은 NULL 값이 대부분이거나 중복된 정보를 포함
- 데이터베이스 스키마가 불필요하게 복잡해지고 저장 공간이 낭비됨

## Resolution Steps
1. 다음 칼럼들을 데이터베이스 스키마에서 제거:
   - **Sessions 테이블**: `visitorId` (원래 NULL인 열)
   - **Totals 테이블**: `transactionRevenue` (totalTransactionRevenue로 대체됨)
   - **Traffic 테이블**: `isVideoAd` (모두 FALSE이거나 NULL)
   - **DeviceGeo 테이블**: `isMobile` (deviceCategory로 대체됨), `javaEnabled` (모두 NULL), `networkDomain` (분석에 불필요)
   - **Hits 테이블**: `dataSource` (제외 사유 불명확)
   - **HitsProduct 테이블**: `localProductPrice` (productPrice로 충분)

2. `schema.sql` 파일에서 해당 칼럼 정의 제거

3. `data_processors.py` 파일에서 해당 칼럼 관련 처리 로직 제거:
   - `_process_sessions_data` 함수에서 visitorId 칼럼 제거
   - `_process_totals_data` 함수에서 transactionRevenue 칼럼 제거
   - `_process_traffic_data` 함수에서 isVideoAd 칼럼 제거
   - `_process_devicegeo_data` 함수에서 isMobile, javaEnabled, networkDomain 칼럼 제거
   - `_process_hits_data` 함수에서 dataSource 칼럼 제거
   - `_process_products_data` 함수에서 localProductPrice 칼럼 제거

## Error Message / Logs
해당 사항 없음. 스키마 변경은 오류 없이 완료되었습니다.

## Related Commits or Pull Requests
- schema.sql 파일 수정: 불필요한 칼럼 제거
- data_processors.py 파일 수정: 해당 칼럼 처리 로직 제거

## Reproduction Steps
해당 사항 없음. 이는 계획된 스키마 변경이었습니다.

## Prevention / Lessons Learned
- 데이터 모델링 시 모든 가용 필드를 무조건 포함시키기보다 실제 사용 사례에 맞게 필요한 필드만 선택
- 정기적인 데이터 모델 검토를 통해 불필요한 칼럼 식별 및 제거
- 각 칼럼의 사용 목적과 가치를 명확히 문서화하여 향후 결정에 참고
- NULL 값이 많은 칼럼은 정기적으로 검토하여 필요성 재평가

## Related Links
- Google Analytics 데이터 스키마 문서: https://support.google.com/analytics/answer/3437719
- 데이터베이스 정규화 및 최적화 가이드: https://learn.microsoft.com/ko-kr/azure/architecture/best-practices/data-modeling 

---

## Metadata
- Timestamp: 2025-07-14 17:30:00 KST
- Severity: Medium
- Impacted Systems: storeToSQL Azure Function, BigQuery 쿼리
- Tags: BigQuery, SQL, 문법 오류, OFFSET

## Problem Summary
Azure Function 실행 시 BigQuery 쿼리에서 문법 오류가 발생했습니다. 오류 메시지는 "Syntax error: Expected end of input but got keyword OFFSET at [135:9]"로, OFFSET 키워드 사용 방법에 문제가 있었습니다.

## Root Cause
BigQuery에서 OFFSET 구문을 사용할 때는 일반적으로 ORDER BY 구문이 필요합니다. 현재 쿼리에서는 WHERE 구문 뒤에 바로 OFFSET을 사용하고 있어 문법 오류가 발생했습니다.

BigQuery의 SQL 문법에 따르면:
1. OFFSET은 ORDER BY 구문 뒤에 사용되어야 합니다.
2. ORDER BY 없이 OFFSET을 사용하면 문법 오류가 발생합니다.

## Resolution Steps
1. queries.py 파일에서 BigQuery 쿼리를 수정했습니다:
   ```sql
   WHERE _TABLE_SUFFIX = '{BQ_DATE_SUFFIX}'
   ORDER BY t.visitStartTime  -- 이 줄 추가
   OFFSET {BQ_OFFSET}
   LIMIT {BQ_LIMIT}
   ```

2. ORDER BY 구문을 추가하여 결과를 방문 시작 시간(visitStartTime)으로 정렬하도록 했습니다.

## Error Message / Logs
```
[2025-07-14T08:24:49.421Z] ? 오류 발생 (메인 함수): 400 Syntax error: Expected end of input but got keyword OFFSET at [135:9]; reason: invalidQuery, location: query, message: Syntax error: Expected end of input but got keyword OFFSET at [135:9]

Location: US
Job ID: beb9b7b8-8f36-4bae-8cc9-3a39ce4d361d
```

## Related Commits or Pull Requests
- queries.py 파일 수정: BigQuery 쿼리에 ORDER BY 구문 추가

## Reproduction Steps
1. 수정 전 쿼리로 Azure Function 실행
2. BigQuery API 호출 시 문법 오류 발생
3. 로그에서 "Syntax error: Expected end of input but got keyword OFFSET" 오류 확인

## Prevention / Lessons Learned
- BigQuery SQL 문법은 표준 SQL과 약간 다를 수 있으므로 공식 문서 참조 필요
- OFFSET 사용 시 항상 ORDER BY 구문을 함께 사용해야 함
- 쿼리 변경 시 로컬에서 테스트 후 배포 권장
- BigQuery의 문법 규칙을 팀 내 공유하여 유사한 오류 방지

## Related Links
- BigQuery SQL 문법 가이드: https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax
- ORDER BY와 OFFSET 사용법: https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#limit_and_offset_clauses 

---

## Metadata
- Timestamp: 2025-07-14 17:30:00 KST
- Severity: Medium
- Impacted Systems: storeToSQL Azure Function, BigQuery
- Tags: SQL 문법, OFFSET, ORDER BY, 서브쿼리

## Problem Summary
BigQuery에서 데이터를 조회할 때 "Syntax error: Expected end of input but got keyword OFFSET at [136:9]" 오류가 발생했습니다. 이는 BigQuery에서 OFFSET 사용 시 특정 규칙을 준수해야 하는데, 이를 위반했기 때문입니다.

## Root Cause
- BigQuery에서 UNNEST와 함께 복잡한 쿼리를 사용할 때 OFFSET을 직접 사용하면 문법 오류가 발생할 수 있습니다.
- 이전에 ORDER BY 구문을 추가했지만, UNNEST를 포함한 복잡한 쿼리 구조에서는 추가적인 처리가 필요했습니다.
- BigQuery는 복잡한 쿼리에서 OFFSET과 LIMIT을 사용할 때 서브쿼리나 WITH 절을 통해 중간 결과를 명확하게 정의해야 합니다.

## Resolution Steps
1. 기존 쿼리를 WITH 절을 사용한 CTE(Common Table Expression)로 재구성했습니다.
2. 메인 쿼리를 base_data라는 CTE로 감싸고, 그 결과에 대해 OFFSET과 LIMIT을 적용했습니다.
3. queries.py 파일의 get_analytics_data_query 메소드를 수정했습니다:
   ```sql
   WITH base_data AS (
       -- 기존 쿼리 내용
       ...
       WHERE _TABLE_SUFFIX = '{BQ_DATE_SUFFIX}'
       ORDER BY t.visitStartTime
   )
   
   SELECT * FROM base_data
   LIMIT {BQ_LIMIT}
   OFFSET {BQ_OFFSET}
   ```

## Error Message / Logs
```
[2025-07-14T08:30:25.371Z] ? 오류 발생 (메인 함수): 400 Syntax error: Expected end of input but got keyword OFFSET at [136:9]; reason: invalidQuery, location: query, message: Syntax error: Expected end of input but got keyword OFFSET at [136:9]

Location: US
Job ID: d448d9c1-b274-49a1-a060-e2219601c638
```

## Related Commits or Pull Requests
- queries.py 파일 수정: WITH 절을 사용하여 쿼리 구조 개선

## Reproduction Steps
1. BigQuery에서 UNNEST를 포함한 복잡한 쿼리 작성
2. ORDER BY 구문 추가 후 OFFSET 사용
3. 쿼리 실행 시 문법 오류 발생

## Prevention / Lessons Learned
- BigQuery에서 복잡한 쿼리(특히 UNNEST를 포함한)를 작성할 때는 서브쿼리나 WITH 절을 사용하여 중간 결과를 명확히 정의해야 합니다.
- OFFSET과 LIMIT을 사용할 때는 항상 ORDER BY와 함께 사용해야 하며, 복잡한 쿼리에서는 서브쿼리로 감싸는 것이 안전합니다.
- BigQuery의 문법은 표준 SQL과 차이가 있으므로, BigQuery 특화 문법과 제약사항을 이해하는 것이 중요합니다.

## Related Links
- BigQuery 표준 SQL 문법 가이드: https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax
- BigQuery OFFSET 및 LIMIT 사용 가이드: https://cloud.google.com/bigquery/docs/paginated-results 